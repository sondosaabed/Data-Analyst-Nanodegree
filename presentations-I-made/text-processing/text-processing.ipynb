{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Text processing pipeline </div>\n",
    "Clean and prepare text for classification tasks and others.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/df752feb-6081-4318-a3a5-125a1d4c68d4\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\">  Pipeline of handling text data sets </div>\n",
    "\n",
    "<div align =\"center\"> \n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b7baeb0c-92dc-4cd1-b99a-8dbe9bb7f9d0\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Tools </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/c15b4b04-fc4c-4d09-97e9-1ff0d1697186\" height=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: Ham or Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the data from the source file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam  \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam  \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam  \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./email_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a sample of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi Walid,\n",
    "\n",
    "Do you listen to music on Spotify, YouTube, Amazon or Apple?\n",
    "\n",
    "If you do - you qualify!\n",
    "\n",
    "You could be making $50 for every song you stream...\n",
    "\n",
    "All it takes is 3 steps...\n",
    "\n",
    "Step 1: Create Your Account\n",
    "Create your account here\n",
    "\n",
    "Step 2: Pick Your Favourite Artist\n",
    "Select from thousands of artists and vibe to the music\n",
    "\n",
    "Step 3: Get Paid\n",
    "That's it, for every song you stream...\n",
    "\n",
    "=> Click here right now to start instantly\n",
    "\n",
    "Regards,\n",
    "\n",
    "Alex\n",
    "\n",
    "---\n",
    "?? Connect with us on Telegram: https://t.me/moneymakingcentral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Preprocessing techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/47bf60bf-3078-47c1-bb58-2f72b9c9a9f2\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rare word removal\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Reduce features\n",
    "- Cleaner, more representative datasets\n",
    "- **Improving Data Quality** Removing noise and irrelevant information ensures that the data fed into the model is clean and consistent.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization**\n",
    "- Tokens or words are extracted from text\n",
    "- Tokenization using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [hi, james, ,, have, you, claim, your, complim...  \n",
       "1  [alt_text, congratulations, ,, you, just, earn...  \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...  \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'] = df['text'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear',\n",
       " 'maryam',\n",
       " ',',\n",
       " 'i',\n",
       " 'would',\n",
       " 'like',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'your',\n",
       " 'application',\n",
       " 'for',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'child',\n",
       " 'protection',\n",
       " 'in',\n",
       " 'emergencies',\n",
       " 'specialist',\n",
       " ',',\n",
       " 'maiduguri',\n",
       " '-',\n",
       " 'nigeria',\n",
       " 'and',\n",
       " 'for',\n",
       " 'your',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'plan',\n",
       " 'international',\n",
       " '.',\n",
       " 'we',\n",
       " 'regret',\n",
       " 'to',\n",
       " 'inform',\n",
       " 'you',\n",
       " 'that',\n",
       " 'on',\n",
       " 'this',\n",
       " 'occasion',\n",
       " 'you',\n",
       " 'have',\n",
       " 'not',\n",
       " 'been',\n",
       " 'successful',\n",
       " 'in',\n",
       " 'your',\n",
       " 'application',\n",
       " '.',\n",
       " 'however',\n",
       " ',',\n",
       " 'there',\n",
       " 'may',\n",
       " 'be',\n",
       " 'other',\n",
       " 'positions',\n",
       " 'which',\n",
       " 'are',\n",
       " 'more',\n",
       " 'suitable',\n",
       " 'for',\n",
       " 'your',\n",
       " 'skills',\n",
       " '.',\n",
       " 'please',\n",
       " 'feel',\n",
       " 'free',\n",
       " 'to',\n",
       " 'view',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'current',\n",
       " 'vacancies',\n",
       " 'on',\n",
       " 'our',\n",
       " 'we']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stop word removal**\n",
    "- Eliminate common words that do not contribute to the meaning\n",
    "- Stop words: \"a\", \"the\", \"and\", \"or\", and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'book', '.', 'love', 'read', 'books', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...  \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'sathya',\n",
       " 'narayanan',\n",
       " ',',\n",
       " 'team',\n",
       " 'reply',\n",
       " 'ticket',\n",
       " '2023-06-17',\n",
       " '07',\n",
       " '05',\n",
       " '04',\n",
       " '.',\n",
       " 'see',\n",
       " 'reply',\n",
       " 'please',\n",
       " 'click',\n",
       " 'https',\n",
       " '//offers',\n",
       " '.',\n",
       " 'cpx-research',\n",
       " '.',\n",
       " 'com/ticket',\n",
       " '.',\n",
       " 'php',\n",
       " '?',\n",
       " 'tid=238212&hash=shmykfq0usb8lluap635y67lr1yigcsiqwn8but5gcoznqn69qquabrgpbmrcblui34exlazilz6ks4ebbxpdyyoiznxnhiprslu2wctvp8acvaqayvm0ul0yccaga3qjjw7rbrmhqyu0upl2xdmq',\n",
       " 'cpx',\n",
       " 'research',\n",
       " 'customer',\n",
       " 'happiness',\n",
       " 'team',\n",
       " 'ps',\n",
       " 'always',\n",
       " 'read',\n",
       " 'answer',\n",
       " 'surveys',\n",
       " 'careful',\n",
       " ',',\n",
       " 'might',\n",
       " 'hidden',\n",
       " 'test',\n",
       " 'questions',\n",
       " 'checking',\n",
       " 'pay',\n",
       " 'attention',\n",
       " '.',\n",
       " 'also',\n",
       " 'youre',\n",
       " 'replying',\n",
       " 'fast',\n",
       " ',',\n",
       " 'partners',\n",
       " 'pay',\n",
       " 'reward',\n",
       " '.',\n",
       " 'tickethash=shmykfq0usb8lluap635y67lr1yigcsiqwn8but5gcoznqn69qquabrgpbmrcblui34exlazilz6ks4ebbxpdyyoiznxnhiprslu2wctvp8acvaqayvm0ul0yccaga3qjjw7rbrmhqyu0upl2xdmq=tickethash']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Stemming**\n",
    "- Reducing words to their base form\n",
    "- For example: \"running\", \"runs\", \"ran\" becomes run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "filtered_tokens = [\"reading\", \"book\", \".\", \"love\", \"read\", \"books\", \"!\"]\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(filtered_tokens):\n",
    "    return [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['steemed_tokens'] = df['remove_stopwords'].apply(stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['steemed_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Rare word removal**\n",
    "- Removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b5c58500-a539-4042-ba89-4b8db816e359\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "stemmed_tokens = [\"read\", \"book\", \".\", \"love\", \"read\", \"book\", \"!\"]\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 1\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare(stemmed_tokens):\n",
    "    freq_dist = FreqDist(stemmed_tokens)\n",
    "    return [token for token in stemmed_tokens if freq_dist[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rare_words_removed'] = df['steemed_tokens'].apply(remove_rare)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the final preprocessed text data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proccessed_text = df['rare_words_removed']\n",
    "proccessed_text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Preprocessing techniques Recap\n",
    "- Tokenization\n",
    "- stopword removal\n",
    "- stemming\n",
    "- rare word removal\n",
    "- More techniques exist\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Encoding techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/7bdbe523-7e07-442b-87b9-e35e602d49f5\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- covert text into machine-readable numbers\n",
    "- Enable analysis and modeling\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/ac62b9af-5c9d-4bc5-a643-3df0ee31c394\" height=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## \n",
    "\n",
    "- Allows models to understand and process text\n",
    "- Choose one technique to avoid redudancy\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Techniques\n",
    "- One-hot encoding: transforms words into unique numerical representations\n",
    "- Bag-of-Words (BoW): captures word frequency, disregarding order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-hot encoding**\n",
    "- Mapping each word to a distinct vector\n",
    "\n",
    "Binary vector:\n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word\n",
    "\n",
    "['cat', 'dog', 'rabbit']\n",
    "\n",
    "'cat' [1, 0, 0]\n",
    "\n",
    "'dog' [0, 1, 0]\n",
    "\n",
    "'rabbit' [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(row):\n",
    "    vocab = set(row)\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot_vectors = torch.eye(vocab_size)\n",
    "    # return {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "    return [one_hot_vectors[i] for i, word in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ohe'] = df['rare_words_removed'].apply(one_hot_encoding)\n",
    "df['ohe'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ohe'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bag of words**\n",
    "- Example: \"The cat sat on the mat\"\n",
    "- Bag-of-words: {'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}\n",
    "\n",
    "- Treating each document as an unordered collection of words\n",
    "-  Focuses on frequency, not order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "- Scores the importance of words in a document\n",
    "- Rare words have a higher score\n",
    "- Common ones have a lower score\n",
    "- Emphasizes informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This document is the second document.', 'And this is the third one.','Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodong Techniques REcap\n",
    "- One hot encoding\n",
    "- Words of bags\n",
    "- TF-IDF encoding\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        threshold = 2\n",
    "        tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_sentences(data):\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(text):\n",
    "    tokens = preprocess_sentences(text)\n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens)\n",
    "    dataset = TextDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"This is the first text data. And here is another one.\"\n",
    "sentences = extract_sentences(text_data)\n",
    "dataloader, vectorizer = [text_processing_pipeline(text) for text in sentences]\n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
