{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Text processing pipeline </div>\n",
    "Clean and prepare text for classification tasks and others.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/df752feb-6081-4318-a3a5-125a1d4c68d4\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\">  Pipeline of handling text data sets </div>\n",
    "\n",
    "<div align =\"center\"> \n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b7baeb0c-92dc-4cd1-b99a-8dbe9bb7f9d0\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Tools </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/c15b4b04-fc4c-4d09-97e9-1ff0d1697186\" height=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: Ham or Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the data from the source file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam  \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam  \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam  \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./email_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a sample of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi Walid,\n",
    "\n",
    "Do you listen to music on Spotify, YouTube, Amazon or Apple?\n",
    "\n",
    "If you do - you qualify!\n",
    "\n",
    "You could be making $50 for every song you stream...\n",
    "\n",
    "All it takes is 3 steps...\n",
    "\n",
    "Step 1: Create Your Account\n",
    "Create your account here\n",
    "\n",
    "Step 2: Pick Your Favourite Artist\n",
    "Select from thousands of artists and vibe to the music\n",
    "\n",
    "Step 3: Get Paid\n",
    "That's it, for every song you stream...\n",
    "\n",
    "=> Click here right now to start instantly\n",
    "\n",
    "Regards,\n",
    "\n",
    "Alex\n",
    "\n",
    "---\n",
    "?? Connect with us on Telegram: https://t.me/moneymakingcentral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Preprocessing techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/47bf60bf-3078-47c1-bb58-2f72b9c9a9f2\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rare word removal\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Reduce features\n",
    "- Cleaner, more representative datasets\n",
    "- **Improving Data Quality** Removing noise and irrelevant information ensures that the data fed into the model is clean and consistent.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization**\n",
    "- Tokens or words are extracted from text\n",
    "- Tokenization using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [hi, james, ,, have, you, claim, your, complim...  \n",
       "1  [alt_text, congratulations, ,, you, just, earn...  \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...  \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'] = df['text'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'sathya',\n",
       " 'narayanan',\n",
       " ',',\n",
       " 'our',\n",
       " 'team',\n",
       " 'reply',\n",
       " 'to',\n",
       " 'your',\n",
       " 'ticket',\n",
       " 'from',\n",
       " '2023-06-17',\n",
       " '07',\n",
       " '05',\n",
       " '04',\n",
       " '.',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'reply',\n",
       " 'please',\n",
       " 'click',\n",
       " 'here',\n",
       " 'https',\n",
       " '//offers',\n",
       " '.',\n",
       " 'cpx-research',\n",
       " '.',\n",
       " 'com/ticket',\n",
       " '.',\n",
       " 'php',\n",
       " '?',\n",
       " 'tid=238212&hash=shmykfq0usb8lluap635y67lr1yigcsiqwn8but5gcoznqn69qquabrgpbmrcblui34exlazilz6ks4ebbxpdyyoiznxnhiprslu2wctvp8acvaqayvm0ul0yccaga3qjjw7rbrmhqyu0upl2xdmq',\n",
       " 'your',\n",
       " 'cpx',\n",
       " 'research',\n",
       " 'customer',\n",
       " 'happiness',\n",
       " 'team',\n",
       " 'ps',\n",
       " 'always',\n",
       " 'read',\n",
       " 'and',\n",
       " 'answer',\n",
       " 'surveys',\n",
       " 'careful',\n",
       " ',',\n",
       " 'there',\n",
       " 'might',\n",
       " 'be',\n",
       " 'hidden',\n",
       " 'test',\n",
       " 'questions',\n",
       " 'checking',\n",
       " 'if',\n",
       " 'you',\n",
       " 'pay',\n",
       " 'attention',\n",
       " '.',\n",
       " 'also',\n",
       " 'if',\n",
       " 'youre',\n",
       " 'replying',\n",
       " 'too',\n",
       " 'fast',\n",
       " ',',\n",
       " 'some',\n",
       " 'partners',\n",
       " 'will',\n",
       " 'not',\n",
       " 'pay',\n",
       " 'your',\n",
       " 'reward',\n",
       " '.',\n",
       " 'tickethash=shmykfq0usb8lluap635y67lr1yigcsiqwn8but5gcoznqn69qquabrgpbmrcblui34exlazilz6ks4ebbxpdyyoiznxnhiprslu2wctvp8acvaqayvm0ul0yccaga3qjjw7rbrmhqyu0upl2xdmq=tickethash']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stop word removal**\n",
    "- Eliminate common words that do not contribute to the meaning\n",
    "- Stop words: \"a\", \"the\", \"and\", \"or\", and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'book', '.', 'love', 'read', 'books', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...  \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear',\n",
       " 'joseph',\n",
       " 'alex',\n",
       " 'eze',\n",
       " 'pleased',\n",
       " 'inform',\n",
       " 'part',\n",
       " 'unicaf’s',\n",
       " '10-year',\n",
       " 'anniversary',\n",
       " ',',\n",
       " 'selected',\n",
       " 'special',\n",
       " 'scholarship',\n",
       " 'allow',\n",
       " 'study',\n",
       " 'towards',\n",
       " 'british',\n",
       " 'master’s',\n",
       " 'degree',\n",
       " 'choice',\n",
       " '?',\n",
       " '1',\n",
       " ',',\n",
       " '950',\n",
       " '.',\n",
       " 'excited',\n",
       " 'join',\n",
       " 'us',\n",
       " 'believe',\n",
       " 'program',\n",
       " 'provide',\n",
       " 'skills',\n",
       " 'knowledge',\n",
       " 'need',\n",
       " 'succeed',\n",
       " 'chosen',\n",
       " 'field',\n",
       " '.',\n",
       " '90%',\n",
       " 'graduates',\n",
       " 'employment',\n",
       " 'earned',\n",
       " 'higher',\n",
       " 'salary',\n",
       " '.',\n",
       " 'believe',\n",
       " 'opportunity',\n",
       " 'allow',\n",
       " 'pursue',\n",
       " 'educational',\n",
       " 'goals',\n",
       " 'without',\n",
       " 'financial',\n",
       " 'burden',\n",
       " 'accompanies',\n",
       " 'master',\n",
       " \"'\",\n",
       " 'degree',\n",
       " 'programme',\n",
       " '.',\n",
       " 'encourage',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'offer',\n",
       " 'join',\n",
       " 'us',\n",
       " 'pursuit',\n",
       " 'knowledge',\n",
       " 'academic',\n",
       " 'excellence',\n",
       " 'replying',\n",
       " 'email',\n",
       " '.',\n",
       " 'take',\n",
       " 'advantage',\n",
       " 'offer',\n",
       " 'please',\n",
       " 'quote',\n",
       " 'adviser',\n",
       " 'code',\n",
       " 'ng1950',\n",
       " '.',\n",
       " 'sincerely',\n",
       " ',',\n",
       " 'olusegun',\n",
       " 'onyinyechi',\n",
       " 'scholarship',\n",
       " 'adviser']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Stemming**\n",
    "- Reducing words to their base form\n",
    "- For example: \"running\", \"runs\", \"ran\" becomes run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', '.', 'love', 'read', 'book', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "filtered_tokens = [\"reading\", \"book\", \".\", \"love\", \"read\", \"books\", \"!\"]\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(filtered_tokens):\n",
    "    return [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...  \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'] = df['remove_stopwords'].apply(stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dear',\n",
       " 'maryam',\n",
       " ',',\n",
       " 'would',\n",
       " 'like',\n",
       " 'thank',\n",
       " 'applic',\n",
       " 'role',\n",
       " 'child',\n",
       " 'protect',\n",
       " 'emerg',\n",
       " 'specialist',\n",
       " ',',\n",
       " 'maiduguri',\n",
       " '-',\n",
       " 'nigeria',\n",
       " 'interest',\n",
       " 'plan',\n",
       " 'intern',\n",
       " '.',\n",
       " 'regret',\n",
       " 'inform',\n",
       " 'occas',\n",
       " 'success',\n",
       " 'applic',\n",
       " '.',\n",
       " 'howev',\n",
       " ',',\n",
       " 'may',\n",
       " 'posit',\n",
       " 'suitabl',\n",
       " 'skill',\n",
       " '.',\n",
       " 'pleas',\n",
       " 'feel',\n",
       " 'free',\n",
       " 'view',\n",
       " 'current',\n",
       " 'vacanc']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Rare word removal**\n",
    "- Removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b5c58500-a539-4042-ba89-4b8db816e359\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', 'read', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "stemmed_tokens = [\"read\", \"book\", \".\", \"love\", \"read\", \"book\", \"!\"]\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 1\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare(stemmed_tokens):\n",
    "    freq_dist = FreqDist(stemmed_tokens)\n",
    "    return [token for token in stemmed_tokens if freq_dist[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "      <th>rare_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "      <td>[,, claim, gift, ?, gift, ?, ., &gt;&gt;, claim, &gt;&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "      <td>[,, earn, point, earn, point, ,, ,, ,, hong, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[github, code, github, code, github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "      <td>[,, thank, contact, virtual, reward, center, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "      <td>[,, today, ', day, ,, insid, play, ,, video, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \\\n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...   \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...   \n",
       "\n",
       "                                  rare_words_removed  \n",
       "0  [,, claim, gift, ?, gift, ?, ., >>, claim, >>,...  \n",
       "1  [,, earn, point, earn, point, ,, ,, ,, hong, k...  \n",
       "2               [github, code, github, code, github]  \n",
       "3  [,, thank, contact, virtual, reward, center, ....  \n",
       "4  [,, today, ', day, ,, insid, play, ,, video, p...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rare_words_removed'] = df['steemed_tokens'].apply(remove_rare)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the final preprocessed text data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44    [notic, login, ,, alexxuzi, notic, login, devi...\n",
       "38                                            [., ., .]\n",
       "8              [,, ., ,, turkey, ,, turkey, ,, ., ,, .]\n",
       "47    [doordash, order, ,, estim, order, 9, pm, 9, p...\n",
       "36    [., ,, statement, account, (, ), ., pleas, acc...\n",
       "31    [., ., ., sex, ?, ?, ., need, ., need, sex, ., .]\n",
       "59    [netflix, ,, ,, find, inform, request, netflix...\n",
       "17    [pleas, ,, scholarship, allow, degre, ,, ., jo...\n",
       "68    [,, 2023, ,, make, chang, googl, play, term, s...\n",
       "61    [,, file, ., today, ,, file, return, ., file, ...\n",
       "Name: rare_words_removed, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed_text = df['rare_words_removed']\n",
    "proccessed_text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Preprocessing techniques Recap\n",
    "- Tokenization\n",
    "- stopword removal\n",
    "- stemming\n",
    "- rare word removal\n",
    "- More techniques exist\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Encoding techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/7bdbe523-7e07-442b-87b9-e35e602d49f5\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- covert text into machine-readable numbers\n",
    "- Enable analysis and modeling\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/ac62b9af-5c9d-4bc5-a643-3df0ee31c394\" height=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## \n",
    "\n",
    "- Allows models to understand and process text\n",
    "- Choose one technique to avoid redudancy\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Techniques\n",
    "- One-hot encoding: transforms words into unique numerical representations\n",
    "- Bag-of-Words (BoW): captures word frequency, disregarding order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-hot encoding**\n",
    "- Mapping each word to a distinct vector\n",
    "\n",
    "Binary vector:\n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word\n",
    "\n",
    "['cat', 'dog', 'rabbit']\n",
    "\n",
    "'cat' [1, 0, 0]\n",
    "\n",
    "'dog' [0, 1, 0]\n",
    "\n",
    "'rabbit' [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': tensor([1., 0., 0.]), 'dog': tensor([0., 1., 0.]), 'rabbit': tensor([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(row):\n",
    "    vocab = set(row)\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot_vectors = torch.eye(vocab_size)\n",
    "    # return {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "    return [one_hot_vectors[i] for i, word in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "1    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "2    [[tensor(1.), tensor(0.)], [tensor(0.), tensor...\n",
       "3    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "4    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "Name: ohe, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'] = df['rare_words_removed'].apply(one_hot_encoding)\n",
    "df['ohe'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 0., 0., 0., 0.]),\n",
       " tensor([0., 1., 0., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bag of words**\n",
    "- Example: \"The cat sat on the mat\"\n",
    "- Bag-of-words: {'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}\n",
    "\n",
    "- Treating each document as an unordered collection of words\n",
    "-  Focuses on frequency, not order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def encode_count_vector(row):\n",
    "    row = ' '.join(row)\n",
    "    return vectorizer.fit_transform([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      (0, 13)\\t1\\n  (0, 14)\\t1\\n  (0, 2)\\t3\\n  (0,...\n",
       "1      (0, 4)\\t1\\n  (0, 11)\\t1\\n  (0, 13)\\t1\\n  (0,...\n",
       "2      (0, 4)\\t3\\n  (0, 5)\\t1\\n  (0, 1)\\t2\\n  (0, 6...\n",
       "3      (0, 10)\\t1\\n  (0, 26)\\t2\\n  (0, 5)\\t1\\n  (0,...\n",
       "4      (0, 174)\\t1\\n  (0, 261)\\t1\\n  (0, 270)\\t1\\n ...\n",
       "Name: cvector_encoded, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cvector_encoded'] = df['remove_stopwords'].apply(encode_count_vector)\n",
    "df['cvector_encoded'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cvector_encoded'][1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "- Scores the importance of words in a document\n",
    "- Rare words have a higher score\n",
    "- Common ones have a lower score\n",
    "- Emphasizes informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This document is the second document.', 'And this is the third one.','Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_1 = TfidfVectorizer()\n",
    "\n",
    "def tf_idf_transform(row):\n",
    "    return vectorizer_1.fit_transform(row).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "1    [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: tfid_vector_encoded, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tfid_vector_encoded'] = df['remove_stopwords'].apply(tf_idf_transform)\n",
    "df['tfid_vector_encoded'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "['90' 'act' 'appear' 'bard' 'click' 'com' 'community' 'company' 'days'\n",
      " 'earned' 'expire' 'explorers' 'follow' 'fulfillment' 'gift' 'go' 'icon'\n",
      " 'india' 'instructions' 'links' 'member' 'online' 'page' 'please'\n",
      " 'profile' 'protection' 'provided' 'questions' 'quickly' 'reach'\n",
      " 'received' 'redeem' 'see' 'sign' 'support' 'top' 'virtualrewardcenter'\n",
      " 'visit']\n"
     ]
    }
   ],
   "source": [
    "print(df['tfid_vector_encoded'][1])\n",
    "print(vectorizer_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodong Techniques REcap\n",
    "- One hot encoding\n",
    "- Words of bags\n",
    "- TF-IDF encoding\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text preparation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        threshold = 2\n",
    "        tokens = [token for token in tokens if freq_dist[token] > threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_sentences(data):\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(text):\n",
    "    tokens = preprocess_sentences(text)\n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens)\n",
    "    dataset = TextDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = \"This is the first text data. And here is another one.\"\n",
    "sentences = extract_sentences(text_data)\n",
    "dataloader, vectorizer = [text_processing_pipeline(text) for text in sentences]\n",
    "print(next(iter(dataloader))[0, :10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
