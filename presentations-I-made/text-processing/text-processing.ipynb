{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Text processing pipeline </div>\n",
    "Clean and prepare text for classification tasks and others.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/df752feb-6081-4318-a3a5-125a1d4c68d4\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\">  Pipeline of handling text data sets </div>\n",
    "\n",
    "<div align =\"center\"> \n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b7baeb0c-92dc-4cd1-b99a-8dbe9bb7f9d0\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Tools </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/c15b4b04-fc4c-4d09-97e9-1ff0d1697186\" height=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: Ham or Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the data from the source file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam  \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam  \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam  \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./email_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a sample of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi Walid,\n",
    "\n",
    "Do you listen to music on Spotify, YouTube, Amazon or Apple?\n",
    "\n",
    "If you do - you qualify!\n",
    "\n",
    "You could be making $50 for every song you stream...\n",
    "\n",
    "All it takes is 3 steps...\n",
    "\n",
    "Step 1: Create Your Account\n",
    "Create your account here\n",
    "\n",
    "Step 2: Pick Your Favourite Artist\n",
    "Select from thousands of artists and vibe to the music\n",
    "\n",
    "Step 3: Get Paid\n",
    "That's it, for every song you stream...\n",
    "\n",
    "=> Click here right now to start instantly\n",
    "\n",
    "Regards,\n",
    "\n",
    "Alex\n",
    "\n",
    "---\n",
    "?? Connect with us on Telegram: https://t.me/moneymakingcentral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Preprocessing techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/47bf60bf-3078-47c1-bb58-2f72b9c9a9f2\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rare word removal\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Reduce features\n",
    "- Cleaner, more representative datasets\n",
    "- **Improving Data Quality** Removing noise and irrelevant information ensures that the data fed into the model is clean and consistent.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization**\n",
    "- Tokens or words are extracted from text\n",
    "- Tokenization using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [hi, james, ,, have, you, claim, your, complim...  \n",
       "1  [alt_text, congratulations, ,, you, just, earn...  \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...  \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'] = df['text'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greetings',\n",
       " 'from',\n",
       " 'sfi',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'keep',\n",
       " 'building',\n",
       " 'something',\n",
       " 'great',\n",
       " '!',\n",
       " 'important',\n",
       " '!',\n",
       " 'if',\n",
       " 'you',\n",
       " 'haven',\n",
       " \"'\",\n",
       " 't',\n",
       " 'checked',\n",
       " 'out',\n",
       " 'our',\n",
       " 'major',\n",
       " 're-shaping',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sfi',\n",
       " 'affiliate',\n",
       " 'program',\n",
       " ',',\n",
       " 'read',\n",
       " 'gery',\n",
       " \"'\",\n",
       " 's',\n",
       " 'detailed',\n",
       " 'announcement',\n",
       " 'now',\n",
       " 'https',\n",
       " '//www',\n",
       " '.',\n",
       " 'sfimg',\n",
       " '.',\n",
       " 'com/forum/thread',\n",
       " '?',\n",
       " 'id=240777',\n",
       " '------------------------------------------------------------------------------------------------------------',\n",
       " 'new',\n",
       " 'genealogy',\n",
       " 'features',\n",
       " '1',\n",
       " '.',\n",
       " 'to',\n",
       " 'make',\n",
       " 'doing',\n",
       " 'mailings',\n",
       " 'to',\n",
       " 'your',\n",
       " 'psas',\n",
       " 'and',\n",
       " 'csas',\n",
       " 'super',\n",
       " 'easy',\n",
       " ',',\n",
       " 'we',\n",
       " \"'\",\n",
       " 've',\n",
       " 'added',\n",
       " 'small',\n",
       " 'envelope',\n",
       " 'icons',\n",
       " 'to',\n",
       " 'the',\n",
       " 'genealogy',\n",
       " 'summary',\n",
       " 'panel',\n",
       " '.',\n",
       " '•',\n",
       " 'to',\n",
       " 'send',\n",
       " 'a',\n",
       " 'group',\n",
       " 'mailing',\n",
       " 'to',\n",
       " 'your',\n",
       " 'psas',\n",
       " ',',\n",
       " 'just',\n",
       " 'click',\n",
       " 'the',\n",
       " 'envelope',\n",
       " 'icon',\n",
       " 'beside',\n",
       " 'your',\n",
       " 'psas',\n",
       " '.',\n",
       " '•',\n",
       " 'to',\n",
       " 'send',\n",
       " 'a',\n",
       " 'group',\n",
       " 'mailing',\n",
       " 'to',\n",
       " 'your',\n",
       " 'csas',\n",
       " ',',\n",
       " 'click',\n",
       " 'the',\n",
       " 'envelope',\n",
       " 'icon',\n",
       " 'beside',\n",
       " 'your',\n",
       " 'csas',\n",
       " '.',\n",
       " 'note',\n",
       " 'minimum',\n",
       " 'rank',\n",
       " 'of',\n",
       " 'gold',\n",
       " 'builder',\n",
       " 'required',\n",
       " 'to',\n",
       " 'use',\n",
       " 'the',\n",
       " 'csa',\n",
       " 'mailer',\n",
       " '.',\n",
       " '2',\n",
       " '.',\n",
       " 'with',\n",
       " 'the',\n",
       " 'new',\n",
       " 'system',\n",
       " ',',\n",
       " 'focus',\n",
       " 'should',\n",
       " 'primarily',\n",
       " 'be',\n",
       " 'on',\n",
       " 'those',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'share-tech™',\n",
       " 'network',\n",
       " '.',\n",
       " 'hence',\n",
       " ',',\n",
       " 'effective',\n",
       " 'shortly',\n",
       " ',',\n",
       " 'when',\n",
       " 'you',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'genealogy',\n",
       " ',',\n",
       " 'it',\n",
       " 'will',\n",
       " ',',\n",
       " 'by',\n",
       " 'default',\n",
       " ',',\n",
       " 'display',\n",
       " 'the',\n",
       " 'condensed',\n",
       " 'version',\n",
       " '.',\n",
       " 'the',\n",
       " 'condensed',\n",
       " 'version',\n",
       " 'will',\n",
       " 'now',\n",
       " 'display',\n",
       " 'only',\n",
       " 'eas',\n",
       " '(',\n",
       " 'from',\n",
       " 'previous',\n",
       " 'month',\n",
       " 'and',\n",
       " 'current',\n",
       " 'month',\n",
       " ')',\n",
       " '+',\n",
       " 'affiliates',\n",
       " 'who',\n",
       " 'are',\n",
       " 'still',\n",
       " 'in',\n",
       " 'their',\n",
       " 'free',\n",
       " '30-day',\n",
       " 'trial',\n",
       " 'window',\n",
       " '(',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.',\n",
       " 'those',\n",
       " 'in',\n",
       " 'the',\n",
       " 'share-tech™',\n",
       " 'network',\n",
       " ')',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'wish',\n",
       " 'to',\n",
       " 'see',\n",
       " 'all',\n",
       " 'your',\n",
       " 'affiliates—including',\n",
       " 'those',\n",
       " 'who',\n",
       " \"'\",\n",
       " 've',\n",
       " 'forfeited',\n",
       " 'their',\n",
       " 'share-tech™',\n",
       " 'position',\n",
       " ',',\n",
       " '—just',\n",
       " 'click',\n",
       " 'on',\n",
       " 'the',\n",
       " 'full',\n",
       " 'version',\n",
       " 'genealogy',\n",
       " 'link',\n",
       " '.',\n",
       " '3',\n",
       " '.',\n",
       " 'if',\n",
       " 'you',\n",
       " 'had',\n",
       " 'any',\n",
       " 'poas',\n",
       " ',',\n",
       " 'they',\n",
       " 'are',\n",
       " 'now',\n",
       " 'being',\n",
       " 'converted',\n",
       " 'to',\n",
       " 'psas',\n",
       " '.',\n",
       " 'even',\n",
       " 'more',\n",
       " 'features',\n",
       " 'and',\n",
       " 'modifications',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'genealogy',\n",
       " 'as',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'use',\n",
       " 'as',\n",
       " 'possible',\n",
       " 'are',\n",
       " 'coming',\n",
       " 'soon',\n",
       " '.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stop word removal**\n",
    "- Eliminate common words that do not contribute to the meaning\n",
    "- Stop words: \"a\", \"the\", \"and\", \"or\", and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'book', '.', 'love', 'read', 'books', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...  \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['live',\n",
       " 'happens',\n",
       " '?',\n",
       " '?',\n",
       " 'live',\n",
       " 'party',\n",
       " 'getting',\n",
       " 'started',\n",
       " '.',\n",
       " 'get',\n",
       " 'hifi',\n",
       " 'hifi',\n",
       " 'plus',\n",
       " 'join',\n",
       " 'fave',\n",
       " 'djs',\n",
       " ',',\n",
       " 'celebs',\n",
       " ',',\n",
       " 'music',\n",
       " 'fans',\n",
       " ',',\n",
       " 'tidal',\n",
       " 'music',\n",
       " 'experts',\n",
       " 'live',\n",
       " '.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Stemming**\n",
    "- Reducing words to their base form\n",
    "- For example: \"running\", \"runs\", \"ran\" becomes run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', '.', 'love', 'read', 'book', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "filtered_tokens = [\"reading\", \"book\", \".\", \"love\", \"read\", \"books\", \"!\"]\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(filtered_tokens):\n",
    "    return [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...  \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'] = df['remove_stopwords'].apply(stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autom',\n",
       " 'messag',\n",
       " 'let',\n",
       " 'know',\n",
       " 'receiv',\n",
       " 'request',\n",
       " 'investig',\n",
       " 'content',\n",
       " 'microsoft',\n",
       " 'servic',\n",
       " '.',\n",
       " 'sincer',\n",
       " ',',\n",
       " 'microsoft',\n",
       " 'custom',\n",
       " 'protect']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Rare word removal**\n",
    "- Removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b5c58500-a539-4042-ba89-4b8db816e359\" height=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', 'read', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "stemmed_tokens = [\"read\", \"book\", \".\", \"love\", \"read\", \"book\", \"!\"]\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 2\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] >= threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare(stemmed_tokens):\n",
    "    freq_dist = FreqDist(stemmed_tokens)\n",
    "    return [token for token in stemmed_tokens if freq_dist[token] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "      <th>rare_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\r\\n\\r\\nHave you claim your complimen...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "      <td>[,, claim, gift, ?, gift, ?, ., &gt;&gt;, claim, &gt;&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\r\\nalt_text\\r\\nCongratulations, you just earn...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "      <td>[,, earn, point, earn, point, ,, ,, ,, hong, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\r\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[github, code, github, code, github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\r\\n \\r\\nThank you for contacting the Vi...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "      <td>[,, thank, contact, virtual, reward, center, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "      <td>[,, today, ', day, ,, insid, play, ,, video, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\r\\n\\r\\nHave you claim your complimen...      spam   \n",
       "1  \\r\\nalt_text\\r\\nCongratulations, you just earn...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\r\\...  not spam   \n",
       "3  Hello,\\r\\n \\r\\nThank you for contacting the Vi...  not spam   \n",
       "4  Hey Prachanda Rawal,\\r\\n\\r\\nToday's newsletter...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \\\n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...   \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...   \n",
       "\n",
       "                                  rare_words_removed  \n",
       "0  [,, claim, gift, ?, gift, ?, ., >>, claim, >>,...  \n",
       "1  [,, earn, point, earn, point, ,, ,, ,, hong, k...  \n",
       "2               [github, code, github, code, github]  \n",
       "3  [,, thank, contact, virtual, reward, center, ....  \n",
       "4  [,, today, ', day, ,, insid, play, ,, video, p...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rare_words_removed'] = df['steemed_tokens'].apply(remove_rare)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the final preprocessed text data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61    [,, file, ., today, ,, file, return, ., file, ...\n",
       "2                  [github, code, github, code, github]\n",
       "31    [., ., ., sex, ?, ?, ., need, ., need, sex, ., .]\n",
       "59    [netflix, ,, ,, find, inform, request, netflix...\n",
       "0     [,, claim, gift, ?, gift, ?, ., >>, claim, >>,...\n",
       "4     [,, today, ', day, ,, insid, play, ,, video, p...\n",
       "64                 [tv, ., ', tv, ', tv, ., ., ., ', .]\n",
       "16    [quick, pay, survey®, ,, ,, account, !, balanc...\n",
       "14    [discog, marketplac, ,, ., seller, discog, fra...\n",
       "45    [,, mani, contribut, develop, bard, ., 1000, i...\n",
       "Name: rare_words_removed, dtype: object"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed_text = df['rare_words_removed']\n",
    "proccessed_text.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deni', 'amazon', 'account', 'deni', '.', '.', 'com', 'link', 'twitch', 'account', '.', 'link', 'twitch', 'account', ',', 'someon', 'els', 'may', 'access', 'account', '.', 'someon', 'els', 'may', 'access', 'account', ',', 'pleas', 'amazon', '.', 'com', '.', 'link', 'amazon', 'twitch', 'account', ',', 'twitch', 'account', '.', 'us', 'account', '.', 'amazon', 'support', 'email', 'email', '.', 'pleas', ',', 'us', '.', 'support', 'amazon', '.', 'com', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(proccessed_text[67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Preprocessing techniques Recap\n",
    "- Tokenization\n",
    "- stopword removal\n",
    "- stemming\n",
    "- rare word removal\n",
    "- More techniques exist\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Encoding techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/7bdbe523-7e07-442b-87b9-e35e602d49f5\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- covert text into machine-readable numbers\n",
    "- Enable analysis and modeling\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/ac62b9af-5c9d-4bc5-a643-3df0ee31c394\" height=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## \n",
    "\n",
    "- Allows models to understand and process text\n",
    "- Choose one technique to avoid redudancy\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Techniques\n",
    "- One-hot encoding: transforms words into unique numerical representations\n",
    "- Bag-of-Words (BoW): captures word frequency, disregarding order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-hot encoding**\n",
    "- Mapping each word to a distinct vector\n",
    "\n",
    "Binary vector:\n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word\n",
    "\n",
    "['cat', 'dog', 'rabbit']\n",
    "\n",
    "'cat' [1, 0, 0]\n",
    "\n",
    "'dog' [0, 1, 0]\n",
    "\n",
    "'rabbit' [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': tensor([1., 0., 0.]), 'dog': tensor([0., 1., 0.]), 'rabbit': tensor([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "one_hot_vectors = torch.eye(vocab_size) ## identity matrix of size vocab_size\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(row):\n",
    "    vocab = set(row)\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot_vectors = torch.eye(vocab_size)\n",
    "    # return {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "    return [one_hot_vectors[i] for i, word in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "1    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "2    [[tensor(1.), tensor(0.)], [tensor(0.), tensor...\n",
       "3    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "4    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "Name: ohe, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'] = df['rare_words_removed'].apply(one_hot_encoding)\n",
    "df['ohe'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 0., 0., 0., 0.]),\n",
       " tensor([0., 1., 0., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bag of words**\n",
    "- Example: \"The cat sat on the mat\"\n",
    "- Bag-of-words: {'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}\n",
    "\n",
    "- Treating each document as an unordered collection of words\n",
    "-  Focuses on frequency, not order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.', \n",
    "          'This document is the second document.', \n",
    "          'And this is the third one.', \n",
    "          'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "def encode_count_vector(row):\n",
    "    row = ' '.join(row)\n",
    "    return vectorizer.fit_transform([row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      (0, 13)\\t1\\n  (0, 14)\\t1\\n  (0, 2)\\t3\\n  (0,...\n",
       "1      (0, 4)\\t1\\n  (0, 11)\\t1\\n  (0, 13)\\t1\\n  (0,...\n",
       "2      (0, 4)\\t3\\n  (0, 5)\\t1\\n  (0, 1)\\t2\\n  (0, 6...\n",
       "3      (0, 10)\\t1\\n  (0, 26)\\t2\\n  (0, 5)\\t1\\n  (0,...\n",
       "4      (0, 174)\\t1\\n  (0, 261)\\t1\\n  (0, 270)\\t1\\n ...\n",
       "Name: cvector_encoded, dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cvector_encoded'] = df['remove_stopwords'].apply(encode_count_vector)\n",
    "df['cvector_encoded'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cvector_encoded'][1].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "- Scores the importance of words in a document\n",
    "- Rare words have a higher score\n",
    "- Common ones have a lower score\n",
    "- Emphasizes informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.', \n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_1 = TfidfVectorizer()\n",
    "\n",
    "def tf_idf_transform(row):\n",
    "    return vectorizer_1.fit_transform(row).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "1    [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "2    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "3    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "4    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: tfid_vector_encoded, dtype: object"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tfid_vector_encoded'] = df['remove_stopwords'].apply(tf_idf_transform)\n",
    "df['tfid_vector_encoded'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n",
      "['90' 'act' 'appear' 'bard' 'click' 'com' 'community' 'company' 'days'\n",
      " 'earned' 'expire' 'explorers' 'follow' 'fulfillment' 'gift' 'go' 'icon'\n",
      " 'india' 'instructions' 'links' 'member' 'online' 'page' 'please'\n",
      " 'profile' 'protection' 'provided' 'questions' 'quickly' 'reach'\n",
      " 'received' 'redeem' 'see' 'sign' 'support' 'top' 'virtualrewardcenter'\n",
      " 'visit']\n"
     ]
    }
   ],
   "source": [
    "print(df['tfid_vector_encoded'][1])\n",
    "print(vectorizer_1.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodong Techniques REcap\n",
    "- One hot encoding\n",
    "- Words of bags\n",
    "- TF-IDF encoding\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Dataset and Dataloader </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/user-attachments/assets/cd30dad1-32ff-4d9b-8c47-fbea8090db48\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    "- dataset as container\n",
    "- dataloader: batching, shuffling, multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a class ### inhertince\n",
    "class TextDataset(Dataset): \n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.text[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Full Text preparation pipeline </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/user-attachments/assets/19a0275c-cd95-48a1-adef-2a75dcc1dfa0\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sentences(data):\n",
    "    sentences = re.findall(r'[A-Z][^.!?]*[.!?]', data)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    processed_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        tokens = tokenizer(sentence)\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        threshold = 1\n",
    "        tokens = [token for token in tokens if freq_dist[token] >= threshold]\n",
    "        processed_sentences.append(' '.join(tokens))\n",
    "    \n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    encoded_sentences = X.toarray()\n",
    "    return encoded_sentences, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(text):\n",
    "    tokens = preprocess_sentences(text)\n",
    "    encoded_sentences, vectorizer = encode_sentences(tokens)\n",
    "    dataset = TextDataset(encoded_sentences)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "    return dataloader, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "text_data = \"This is the first text data. It contains several sentences. Here is another one. This text is a bit longer to illustrate the process.\"\n",
    "sentences = extract_sentences(text_data)\n",
    "dataloader, vectorizer = text_processing_pipeline(sentences)\n",
    "print(next(iter(dataloader))[0])\n",
    "print(next(iter(dataloader))[0])\n",
    "print(next(iter(dataloader))[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
