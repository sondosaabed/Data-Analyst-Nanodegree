{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Text processing pipeline </div>\n",
    "Clean and prepare text for classification tasks and others.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/df752feb-6081-4318-a3a5-125a1d4c68d4\" height=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\">  Pipeline of handling text data sets </div>\n",
    "\n",
    "<div align =\"center\"> \n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b7baeb0c-92dc-4cd1-b99a-8dbe9bb7f9d0\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Tools </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/c15b4b04-fc4c-4d09-97e9-1ff0d1697186\" height=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: Ham or Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the data from the source file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam  \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam  \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam  \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./email_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a sample of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi Walid,\n",
    "\n",
    "Do you listen to music on Spotify, YouTube, Amazon or Apple?\n",
    "\n",
    "If you do - you qualify!\n",
    "\n",
    "You could be making $50 for every song you stream...\n",
    "\n",
    "All it takes is 3 steps...\n",
    "\n",
    "Step 1: Create Your Account\n",
    "Create your account here\n",
    "\n",
    "Step 2: Pick Your Favourite Artist\n",
    "Select from thousands of artists and vibe to the music\n",
    "\n",
    "Step 3: Get Paid\n",
    "That's it, for every song you stream...\n",
    "\n",
    "=> Click here right now to start instantly\n",
    "\n",
    "Regards,\n",
    "\n",
    "Alex\n",
    "\n",
    "---\n",
    "?? Connect with us on Telegram: https://t.me/moneymakingcentral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Preprocessing techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/47bf60bf-3078-47c1-bb58-2f72b9c9a9f2\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rare word removal\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Reduce features\n",
    "- Cleaner, more representative datasets\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization**\n",
    "- Tokens or words are extracted from text\n",
    "- Tokenization using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [hi, james, ,, have, you, claim, your, complim...  \n",
       "1  [alt_text, congratulations, ,, you, just, earn...  \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...  \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'] = df['text'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'casting',\n",
       " 'call',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'for',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'time',\n",
       " 'to',\n",
       " 'register',\n",
       " 'for',\n",
       " 'the',\n",
       " 'anambra',\n",
       " 'fashion',\n",
       " 'expo',\n",
       " '2023',\n",
       " 'model',\n",
       " 'call',\n",
       " '.',\n",
       " 'we',\n",
       " 'are',\n",
       " 'thrilled',\n",
       " 'to',\n",
       " 'have',\n",
       " 'received',\n",
       " 'your',\n",
       " 'information',\n",
       " 'and',\n",
       " 'are',\n",
       " 'excited',\n",
       " 'to',\n",
       " 'review',\n",
       " 'your',\n",
       " 'submission',\n",
       " '.',\n",
       " 'our',\n",
       " 'team',\n",
       " 'will',\n",
       " 'be',\n",
       " 'carefully',\n",
       " 'reviewing',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'applications',\n",
       " 'we',\n",
       " 'receive',\n",
       " 'over',\n",
       " 'the',\n",
       " 'next',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'have',\n",
       " 'you',\n",
       " 'followed',\n",
       " 'us',\n",
       " 'on',\n",
       " 'our',\n",
       " 'social',\n",
       " 'media',\n",
       " 'handles',\n",
       " '?',\n",
       " 'remember',\n",
       " ',',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'prerequisites',\n",
       " 'for',\n",
       " 'qualification',\n",
       " 'is',\n",
       " 'to',\n",
       " 'follow',\n",
       " 'all',\n",
       " 'our',\n",
       " 'social',\n",
       " 'media',\n",
       " 'accounts',\n",
       " 'and',\n",
       " 'share',\n",
       " 'all',\n",
       " 'our',\n",
       " 'content',\n",
       " 'using',\n",
       " 'the',\n",
       " 'hashtag',\n",
       " '#afe2023',\n",
       " 'you',\n",
       " 'can',\n",
       " 'follow',\n",
       " 'us',\n",
       " 'on',\n",
       " 'facebook',\n",
       " ',',\n",
       " 'instagram',\n",
       " ',',\n",
       " 'and',\n",
       " 'twitter',\n",
       " '.',\n",
       " 'facebook',\n",
       " 'https',\n",
       " '//facebook',\n",
       " '.',\n",
       " 'com/anambrafashionexpo2023',\n",
       " 'instagram',\n",
       " 'https',\n",
       " '//instagram',\n",
       " '.',\n",
       " 'com/anambrafashionexpo',\n",
       " 'twitter',\n",
       " 'https',\n",
       " '//twitter',\n",
       " '.',\n",
       " 'com/anambrafashion',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " ',',\n",
       " 'we',\n",
       " 'encourage',\n",
       " 'you',\n",
       " 'to',\n",
       " 'stay',\n",
       " 'connected',\n",
       " 'and',\n",
       " 'keep',\n",
       " 'an',\n",
       " 'eye',\n",
       " 'out',\n",
       " 'for',\n",
       " 'updates',\n",
       " 'about',\n",
       " 'the',\n",
       " 'event',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'be',\n",
       " 'posting',\n",
       " '!',\n",
       " 'note',\n",
       " 'create',\n",
       " 'your',\n",
       " 'personalized',\n",
       " 'profile',\n",
       " 'picture',\n",
       " '(',\n",
       " 'dp',\n",
       " ')',\n",
       " 'for',\n",
       " 'the',\n",
       " 'model',\n",
       " 'casting',\n",
       " 'of',\n",
       " 'the',\n",
       " 'anambra',\n",
       " 'fashion',\n",
       " 'expo',\n",
       " '2023',\n",
       " 'you',\n",
       " 'can',\n",
       " 'create',\n",
       " 'your',\n",
       " 'dp',\n",
       " 'using',\n",
       " 'the',\n",
       " 'following',\n",
       " 'link',\n",
       " 'https',\n",
       " '//getdp',\n",
       " '.',\n",
       " 'co/afe2023',\n",
       " 'best',\n",
       " 'regards',\n",
       " ',',\n",
       " 'anambra',\n",
       " 'fashion',\n",
       " 'expo',\n",
       " '2023',\n",
       " 'team']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Stop word removal**\n",
    "- Eliminate common words that do not contribute to the meaning\n",
    "- Stop words: \"a\", \"the\", \"and\", \"or\", and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'book', '.', 'love', 'read', 'books', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...  \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bolt',\n",
       " 'rides',\n",
       " 'affordable',\n",
       " ',',\n",
       " 'also',\n",
       " 'come',\n",
       " 'number',\n",
       " 'safety',\n",
       " 'features',\n",
       " '.',\n",
       " 'things',\n",
       " 'ensure',\n",
       " 'safety',\n",
       " 'train',\n",
       " 'drivers',\n",
       " 'verify',\n",
       " 'identity',\n",
       " 'app',\n",
       " '‘share',\n",
       " 'eta’',\n",
       " 'emergency',\n",
       " 'assist',\n",
       " 'buttons',\n",
       " 'use',\n",
       " 'rider',\n",
       " 'feedback',\n",
       " 'improve',\n",
       " 'future',\n",
       " 'journeys',\n",
       " '.',\n",
       " 'learn',\n",
       " 'bolt’s',\n",
       " 'safety',\n",
       " 'measures',\n",
       " '.',\n",
       " 'open',\n",
       " 'app',\n",
       " 'safe',\n",
       " 'travels',\n",
       " '!',\n",
       " '?',\n",
       " '?',\n",
       " 'bolt',\n",
       " 'team',\n",
       " 'n',\n",
       " '.',\n",
       " 'b',\n",
       " '.',\n",
       " 'limited-time',\n",
       " 'offer',\n",
       " 'use',\n",
       " '2023-07-18',\n",
       " '2023-07-24',\n",
       " 'nairobi',\n",
       " ',',\n",
       " 'kenya',\n",
       " '.',\n",
       " 'information',\n",
       " 'app',\n",
       " '.',\n",
       " 'discount',\n",
       " 'valid',\n",
       " '250',\n",
       " 'kes',\n",
       " 'per',\n",
       " 'trip',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Stemming**\n",
    "- Reducing words to their base form\n",
    "- For example: \"running\", \"runs\", \"ran\" becomes run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', '.', 'love', 'read', 'book', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "filtered_tokens = [\"reading\", \"book\", \".\", \"love\", \"read\", \"books\", \"!\"]\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(filtered_tokens):\n",
    "    return [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...  \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'] = df['remove_stopwords'].apply(stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sale',\n",
       " 'execut',\n",
       " 'dear',\n",
       " 'hire',\n",
       " 'profession',\n",
       " 'contact',\n",
       " 'express',\n",
       " 'interest',\n",
       " 'sale',\n",
       " 'execut',\n",
       " 'posit',\n",
       " '.',\n",
       " 'review',\n",
       " 'posit',\n",
       " 'requir',\n",
       " ',',\n",
       " 'believ',\n",
       " 'educ',\n",
       " 'experi',\n",
       " 'great',\n",
       " 'match',\n",
       " 'posit',\n",
       " '.',\n",
       " 'energet',\n",
       " 'decis',\n",
       " 'negoti',\n",
       " 'skill',\n",
       " 'goal-set',\n",
       " ',',\n",
       " 'sale',\n",
       " 'recommend',\n",
       " 'product',\n",
       " '.',\n",
       " 'natur',\n",
       " 'talent',\n",
       " 'build',\n",
       " 'immedi',\n",
       " 'rapport',\n",
       " 'peopl',\n",
       " 'cultiv',\n",
       " 'product',\n",
       " 'connect',\n",
       " '.',\n",
       " 'likewis',\n",
       " ',',\n",
       " 'fulli',\n",
       " 'capabl',\n",
       " 'work',\n",
       " 'strong',\n",
       " 'person',\n",
       " 'navig',\n",
       " 'high-pressur',\n",
       " 'situat',\n",
       " '.',\n",
       " 'detail',\n",
       " ',',\n",
       " 'pleas',\n",
       " 'review',\n",
       " 'attach',\n",
       " 'resum',\n",
       " '.',\n",
       " 'believ',\n",
       " 'sale',\n",
       " 'execut',\n",
       " \"'\",\n",
       " 'look',\n",
       " 'welcom',\n",
       " 'opportun',\n",
       " 'speak',\n",
       " 'earliest',\n",
       " 'conveni',\n",
       " '.',\n",
       " 'sincer',\n",
       " ',',\n",
       " 'zandi',\n",
       " 'taman']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'].sample().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Rare word removal**\n",
    "- Removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![28e3fa6560d2ac80296183f5cea80447-1815626993](https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b5c58500-a539-4042-ba89-4b8db816e359)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', 'read', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "stemmed_tokens = [\"read\", \"book\", \".\", \"love\", \"read\", \"book\", \"!\"]\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 1\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare(stemmed_tokens):\n",
    "    freq_dist = FreqDist(stemmed_tokens)\n",
    "    return [token for token in stemmed_tokens if freq_dist[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "      <th>rare_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "      <td>[,, claim, gift, ?, gift, ?, ., &gt;&gt;, claim, &gt;&gt;,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "      <td>[,, earn, point, earn, point, ,, ,, ,, hong, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[github, code, github, code, github]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "      <td>[,, thank, contact, virtual, reward, center, ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "      <td>[,, today, ', day, ,, insid, play, ,, video, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \\\n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...   \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...   \n",
       "\n",
       "                                  rare_words_removed  \n",
       "0  [,, claim, gift, ?, gift, ?, ., >>, claim, >>,...  \n",
       "1  [,, earn, point, earn, point, ,, ,, ,, hong, k...  \n",
       "2               [github, code, github, code, github]  \n",
       "3  [,, thank, contact, virtual, reward, center, ....  \n",
       "4  [,, today, ', day, ,, insid, play, ,, video, p...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rare_words_removed'] = df['steemed_tokens'].apply(remove_rare)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the final preprocessed text data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59    [netflix, ,, ,, find, inform, request, netflix...\n",
       "81    [vladis163ru, steam, guard, code, login, accou...\n",
       "38                                            [., ., .]\n",
       "44    [notic, login, ,, alexxuzi, notic, login, devi...\n",
       "9     [,, interest, join, appen, !, email, invit, ne...\n",
       "52    [,, team, repli, ., repli, ., ., ., team, ,, p...\n",
       "23                                                   []\n",
       "74                                  [., ., ., ==>, ==>]\n",
       "68    [,, 2023, ,, make, chang, googl, play, term, s...\n",
       "62    [,, paypal, $8, ,, 32, usd, ., transact, trans...\n",
       "Name: rare_words_removed, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed_text = df['rare_words_removed']\n",
    "proccessed_text.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Preprocessing techniques Recap\n",
    "- Tokenization\n",
    "- stopword removal\n",
    "- stemming\n",
    "- rare word removal\n",
    "- More techniques exist\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Encoding techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/7bdbe523-7e07-442b-87b9-e35e602d49f5\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- covert text into machine-readable numbers\n",
    "- Enable analysis and modeling\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/ac62b9af-5c9d-4bc5-a643-3df0ee31c394\" height=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## \n",
    "\n",
    "- Allows models to understand and process text\n",
    "- Choose one technique to avoid redudancy\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Techniques\n",
    "- One-hot encoding: transforms words into unique numerical representations\n",
    "- Bag-of-Words (BoW): captures word frequency, disregarding order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-hot encoding**\n",
    "- Mapping each word to a distinct vector\n",
    "\n",
    "Binary vector:\n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word\n",
    "\n",
    "['cat', 'dog', 'rabbit']\n",
    "\n",
    "'cat' [1, 0, 0]\n",
    "\n",
    "'dog' [0, 1, 0]\n",
    "\n",
    "'rabbit' [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': tensor([1., 0., 0.]), 'dog': tensor([0., 1., 0.]), 'rabbit': tensor([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(row):\n",
    "    vocab = set(row)\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot_vectors = torch.eye(vocab_size)\n",
    "    # return {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "    return [one_hot_vectors[i] for i, word in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "1    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "2    [[tensor(1.), tensor(0.)], [tensor(0.), tensor...\n",
       "3    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "4    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "Name: ohe, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'] = df['rare_words_removed'].apply(one_hot_encoding)\n",
    "df['ohe'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 0., 0., 0., 0.]),\n",
       " tensor([0., 1., 0., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bag of words**\n",
    "- Example: \"The cat sat on the mat\"\n",
    "- Bag-of-words: {'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}\n",
    "\n",
    "- Treating each document as an unordered collection of words\n",
    "-  Focuses on frequency, not order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "- Scores the importance of words in a document\n",
    "- Rare words have a higher score\n",
    "- Common ones have a lower score\n",
    "- Emphasizes informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This document is the second document.', 'And this is the third one.','Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodong Techniques REcap\n",
    "- One hot encoding\n",
    "- Words of bags\n",
    "- TF-IDF encoding\n",
    "- More techniques exist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
