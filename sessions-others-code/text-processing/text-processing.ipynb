{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align=\"center\"> Text processing pipeline </div>\n",
    "Clean and prepare text for classification tasks and others.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\">  Pipeline of handling text data sets </div>\n",
    "\n",
    "<div align =\"center\"> \n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/b7baeb0c-92dc-4cd1-b99a-8dbe9bb7f9d0\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "- Sentiment analysis\n",
    "- Text summarization\n",
    "- Machine translation\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Tools </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/c15b4b04-fc4c-4d09-97e9-1ff0d1697186\" height=\"200\"/>\n",
    "</div>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Dataset: Ham or Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reading the data from the source file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam  \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam  \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam  \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam  \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./email_spam.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a sample of the emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi Walid,\n",
    "\n",
    "Do you listen to music on Spotify, YouTube, Amazon or Apple?\n",
    "\n",
    "If you do - you qualify!\n",
    "\n",
    "You could be making $50 for every song you stream...\n",
    "\n",
    "All it takes is 3 steps...\n",
    "\n",
    "Step 1: Create Your Account\n",
    "Create your account here\n",
    "\n",
    "Step 2: Pick Your Favourite Artist\n",
    "Select from thousands of artists and vibe to the music\n",
    "\n",
    "Step 3: Get Paid\n",
    "That's it, for every song you stream...\n",
    "\n",
    "=> Click here right now to start instantly\n",
    "\n",
    "Regards,\n",
    "\n",
    "Alex\n",
    "\n",
    "---\n",
    "?? Connect with us on Telegram: https://t.me/moneymakingcentral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Preprocessing techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/47bf60bf-3078-47c1-bb58-2f72b9c9a9f2\" height=\"200\">\n",
    "</div>\n",
    "\n",
    "- Tokenization\n",
    "- Stop word removal\n",
    "- Stemming\n",
    "- Rare word removal\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Reduce features\n",
    "- Cleaner, more representative datasets\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Tokenization**\n",
    "- Tokens or words are extracted from text\n",
    "- Tokenization using torchtext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'reading', 'a', 'book', 'now', '.', 'i', 'love', 'to', 'read', 'books', '!']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"I am reading a book now. I love to read books!\")\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \n",
       "0  [hi, james, ,, have, you, claim, your, complim...  \n",
       "1  [alt_text, congratulations, ,, you, just, earn...  \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...  \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_tokens'] = df['text'].apply(tokenizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.Stop word removal**\n",
    "- Eliminate common words that do not contribute to the meaning\n",
    "- Stop words: \"a\", \"the\", \"and\", \"or\", and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reading', 'book', '.', 'love', 'read', 'books', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...  \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['remove_stopwords'] = df['text_tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Stemming**\n",
    "- Reducing words to their base form\n",
    "- For example: \"running\", \"runs\", \"ran\" becomes run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', '.', 'love', 'read', 'book', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "filtered_tokens = [\"reading\", \"book\", \".\", \"love\", \"read\", \"books\", \"!\"]\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(filtered_tokens):\n",
    "    return [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>?? the secrets to SUCCESS</td>\n",
       "      <td>Hi James,\\n\\nHave you claim your complimentary...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hi, james, ,, have, you, claim, your, complim...</td>\n",
       "      <td>[hi, james, ,, claim, complimentary, gift, yet...</td>\n",
       "      <td>[hi, jame, ,, claim, complimentari, gift, yet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>?? You Earned 500 GCLoot Points</td>\n",
       "      <td>\\nalt_text\\nCongratulations, you just earned\\n...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[alt_text, congratulations, ,, you, just, earn...</td>\n",
       "      <td>[alt_text, congratulations, ,, earned, 500, co...</td>\n",
       "      <td>[alt_text, congratul, ,, earn, 500, complet, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?? Your GitHub launch code</td>\n",
       "      <td>Here's your GitHub launch code, @Mortyj420!\\n ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[here, ', s, your, github, launch, code, ,, @m...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "      <td>[', github, launch, code, ,, @mortyj420, !, oc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[The Virtual Reward Center] Re: ** Clarifications</td>\n",
       "      <td>Hello,\\n \\nThank you for contacting the Virtua...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[hello, ,, thank, you, for, contacting, the, v...</td>\n",
       "      <td>[hello, ,, thank, contacting, virtual, reward,...</td>\n",
       "      <td>[hello, ,, thank, contact, virtual, reward, ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10-1 MLB Expert Inside, Plus Everything You Ne...</td>\n",
       "      <td>Hey Prachanda Rawal,\\n\\nToday's newsletter is ...</td>\n",
       "      <td>spam</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', s, newsle...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslette...</td>\n",
       "      <td>[hey, prachanda, rawal, ,, today, ', newslett,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                          ?? the secrets to SUCCESS   \n",
       "1                    ?? You Earned 500 GCLoot Points   \n",
       "2                         ?? Your GitHub launch code   \n",
       "3  [The Virtual Reward Center] Re: ** Clarifications   \n",
       "4  10-1 MLB Expert Inside, Plus Everything You Ne...   \n",
       "\n",
       "                                                text      type  \\\n",
       "0  Hi James,\\n\\nHave you claim your complimentary...      spam   \n",
       "1  \\nalt_text\\nCongratulations, you just earned\\n...  not spam   \n",
       "2  Here's your GitHub launch code, @Mortyj420!\\n ...  not spam   \n",
       "3  Hello,\\n \\nThank you for contacting the Virtua...  not spam   \n",
       "4  Hey Prachanda Rawal,\\n\\nToday's newsletter is ...      spam   \n",
       "\n",
       "                                         text_tokens  \\\n",
       "0  [hi, james, ,, have, you, claim, your, complim...   \n",
       "1  [alt_text, congratulations, ,, you, just, earn...   \n",
       "2  [here, ', s, your, github, launch, code, ,, @m...   \n",
       "3  [hello, ,, thank, you, for, contacting, the, v...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', s, newsle...   \n",
       "\n",
       "                                    remove_stopwords  \\\n",
       "0  [hi, james, ,, claim, complimentary, gift, yet...   \n",
       "1  [alt_text, congratulations, ,, earned, 500, co...   \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...   \n",
       "3  [hello, ,, thank, contacting, virtual, reward,...   \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslette...   \n",
       "\n",
       "                                      steemed_tokens  \n",
       "0  [hi, jame, ,, claim, complimentari, gift, yet,...  \n",
       "1  [alt_text, congratul, ,, earn, 500, complet, f...  \n",
       "2  [', github, launch, code, ,, @mortyj420, !, oc...  \n",
       "3  [hello, ,, thank, contact, virtual, reward, ce...  \n",
       "4  [hey, prachanda, rawal, ,, today, ', newslett,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['steemed_tokens'] = df['remove_stopwords'].apply(stemming)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Rare word removal**\n",
    "- Removing infrequent words that don't add value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['read', 'book', 'read', 'book']\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "stemmed_tokens= [\"read\", \"book\", \".\", \"love\", \"read\", \"book\", \"!\"]\n",
    "freq_dist = FreqDist(stemmed_tokens)\n",
    "threshold = 1\n",
    "\n",
    "common_tokens = [token for token in stemmed_tokens if freq_dist[token] > threshold]\n",
    "print(common_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare(stemmed_tokens):\n",
    "    freq_dist = FreqDist(stemmed_tokens)\n",
    "    return [token for token in stemmed_tokens if freq_dist[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>steemed_tokens</th>\n",
       "      <th>rare_words_removed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Your application for the position of  Child Pr...</td>\n",
       "      <td>Dear Maryam, \\n\\n \\n\\nI would like to thank yo...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[dear, maryam, ,, i, would, like, to, thank, y...</td>\n",
       "      <td>[dear, maryam, ,, would, like, thank, applicat...</td>\n",
       "      <td>[dear, maryam, ,, would, like, thank, applic, ...</td>\n",
       "      <td>[,, applic, ,, ., applic, ., ,, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Your Kilimall Account is Ready - Shopping Now!</td>\n",
       "      <td>Dear Customer,\\n\\nWelcome to Kilimall, Thanks ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[dear, customer, ,, welcome, to, kilimall, ,, ...</td>\n",
       "      <td>[dear, customer, ,, welcome, kilimall, ,, than...</td>\n",
       "      <td>[dear, custom, ,, welcom, kilimal, ,, thank, m...</td>\n",
       "      <td>[custom, ,, kilimal, ,, much, ., kilimal, afri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Your Steam account: Access from new web or mob...</td>\n",
       "      <td>Dear vladis163rus,\\nHere is the Steam Guard co...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[dear, vladis163rus, ,, here, is, the, steam, ...</td>\n",
       "      <td>[dear, vladis163rus, ,, steam, guard, code, ne...</td>\n",
       "      <td>[dear, vladis163ru, ,, steam, guard, code, nee...</td>\n",
       "      <td>[vladis163ru, steam, guard, code, login, accou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Your uploaded document is rejected</td>\n",
       "      <td>View In Browser | Log in\\n \\n \\n\\nSkrill logo\\...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[view, in, browser, |, log, in, skrill, logo, ...</td>\n",
       "      <td>[view, browser, |, log, skrill, logo, money, m...</td>\n",
       "      <td>[view, browser, |, log, skrill, logo, money, m...</td>\n",
       "      <td>[|, skrill, money, ?, ?, couldn’t, verifi, add...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>You've Earned a Reward from Bard Explorers India</td>\n",
       "      <td>You've received a gift!\\nSign in to your Bard ...</td>\n",
       "      <td>not spam</td>\n",
       "      <td>[you, ', ve, received, a, gift, !, sign, in, t...</td>\n",
       "      <td>[', received, gift, !, sign, bard, explorers, ...</td>\n",
       "      <td>[', receiv, gift, !, sign, bard, explor, india...</td>\n",
       "      <td>[gift, !, bard, explor, india, commun, member,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "79  Your application for the position of  Child Pr...   \n",
       "80     Your Kilimall Account is Ready - Shopping Now!   \n",
       "81  Your Steam account: Access from new web or mob...   \n",
       "82                 Your uploaded document is rejected   \n",
       "83   You've Earned a Reward from Bard Explorers India   \n",
       "\n",
       "                                                 text      type  \\\n",
       "79  Dear Maryam, \\n\\n \\n\\nI would like to thank yo...  not spam   \n",
       "80  Dear Customer,\\n\\nWelcome to Kilimall, Thanks ...  not spam   \n",
       "81  Dear vladis163rus,\\nHere is the Steam Guard co...  not spam   \n",
       "82  View In Browser | Log in\\n \\n \\n\\nSkrill logo\\...  not spam   \n",
       "83  You've received a gift!\\nSign in to your Bard ...  not spam   \n",
       "\n",
       "                                          text_tokens  \\\n",
       "79  [dear, maryam, ,, i, would, like, to, thank, y...   \n",
       "80  [dear, customer, ,, welcome, to, kilimall, ,, ...   \n",
       "81  [dear, vladis163rus, ,, here, is, the, steam, ...   \n",
       "82  [view, in, browser, |, log, in, skrill, logo, ...   \n",
       "83  [you, ', ve, received, a, gift, !, sign, in, t...   \n",
       "\n",
       "                                     remove_stopwords  \\\n",
       "79  [dear, maryam, ,, would, like, thank, applicat...   \n",
       "80  [dear, customer, ,, welcome, kilimall, ,, than...   \n",
       "81  [dear, vladis163rus, ,, steam, guard, code, ne...   \n",
       "82  [view, browser, |, log, skrill, logo, money, m...   \n",
       "83  [', received, gift, !, sign, bard, explorers, ...   \n",
       "\n",
       "                                       steemed_tokens  \\\n",
       "79  [dear, maryam, ,, would, like, thank, applic, ...   \n",
       "80  [dear, custom, ,, welcom, kilimal, ,, thank, m...   \n",
       "81  [dear, vladis163ru, ,, steam, guard, code, nee...   \n",
       "82  [view, browser, |, log, skrill, logo, money, m...   \n",
       "83  [', receiv, gift, !, sign, bard, explor, india...   \n",
       "\n",
       "                                   rare_words_removed  \n",
       "79                 [,, applic, ,, ., applic, ., ,, .]  \n",
       "80  [custom, ,, kilimal, ,, much, ., kilimal, afri...  \n",
       "81  [vladis163ru, steam, guard, code, login, accou...  \n",
       "82  [|, skrill, money, ?, ?, couldn’t, verifi, add...  \n",
       "83  [gift, !, bard, explor, india, commun, member,...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rare_words_removed'] = df['steemed_tokens'].apply(remove_rare)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is how the final preprocessed text data would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46                                                   []\n",
       "59    [netflix, ,, ,, find, inform, request, netflix...\n",
       "24    [respond, feedback, feedback, us, ., ', ,, us,...\n",
       "66                       [,, ,, ,, ., ,, ., best, best]\n",
       "29    [,, ,, find, supplement, program, ., supplemen...\n",
       "62    [,, paypal, $8, ,, 32, usd, ., transact, trans...\n",
       "30            [singl, look, ?, ?, ', ', look, ', singl]\n",
       "75    [!, applic, process, job, -, need, (, ), (, jo...\n",
       "73    [?, ?, six, pack, ?, ,, engin, •, answer, ,, !...\n",
       "33    [top, stori, quora, ?, code, c++, hard, ,, man...\n",
       "44    [notic, login, ,, alexxuzi, notic, login, devi...\n",
       "40    [live, ?, ?, live, get, ., get, hifi, hifi, ,,...\n",
       "54    [snapchat+, ,, ', ., snapchat+, plan, set, ren...\n",
       "56                               [microsoft, microsoft]\n",
       "78    [amazon, order, confirm, maliek, ,, shop, ., c...\n",
       "41    [,, zoom, call, -, ,, (, ), ., regist, free, s...\n",
       "53    [sale, execut, sale, execut, posit, ., review,...\n",
       "2                  [github, code, github, code, github]\n",
       "47    [doordash, order, ,, estim, order, 9, pm, 9, p...\n",
       "31    [., ., ., sex, ?, ?, ., need, ., need, sex, ., .]\n",
       "Name: rare_words_removed, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proccessed_text = df['rare_words_removed']\n",
    "proccessed_text.sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Preprocessing techniques Recap\n",
    "- Tokenization\n",
    "- stopword removal\n",
    "- stemming\n",
    "- rare word removal\n",
    "- More techniques exist\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div align =\"center\"> Encoding techniques </div>\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/7bdbe523-7e07-442b-87b9-e35e602d49f5\" height=\"120\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "- covert text into machine-readable numbers\n",
    "- Enable analysis and modeling\n",
    "\n",
    "<div align =\"center\">\n",
    "    <img src=\"https://github.com/sondosaabed/Data-Analyst-Nanodegree/assets/65151701/ac62b9af-5c9d-4bc5-a643-3df0ee31c394\" height=\"500\"/>\n",
    "</div>\n",
    "\n",
    "## \n",
    "\n",
    "- Allows models to understand and process text\n",
    "- Choose one technique to avoid redudancy\n",
    "- More techniques exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Techniques\n",
    "- One-hot encoding: transforms words into unique numerical representations\n",
    "- Bag-of-Words (BoW): captures word frequency, disregarding order\n",
    "- TF-IDF: balances uniqueness and importance\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. One-hot encoding**\n",
    "- Mapping each word to a distinct vector\n",
    "\n",
    "Binary vector:\n",
    "- 1 for the presence of a word\n",
    "- 0 for the absence of a word\n",
    "\n",
    "['cat', 'dog', 'rabbit']\n",
    "\n",
    "'cat' [1, 0, 0]\n",
    "\n",
    "'dog' [0, 1, 0]\n",
    "\n",
    "'rabbit' [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': tensor([1., 0., 0.]), 'dog': tensor([0., 1., 0.]), 'rabbit': tensor([0., 0., 1.])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "vocab = ['cat', 'dog', 'rabbit']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "one_hot_vectors = torch.eye(vocab_size)\n",
    "one_hot_dict = {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "\n",
    "print(one_hot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(row):\n",
    "    vocab = set(row)\n",
    "    vocab_size = len(vocab)\n",
    "    one_hot_vectors = torch.eye(vocab_size)\n",
    "    # return {word: one_hot_vectors[i] for i, word in enumerate(vocab)}\n",
    "    return [one_hot_vectors[i] for i, word in enumerate(vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "1    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "2    [[tensor(1.), tensor(0.)], [tensor(0.), tensor...\n",
       "3    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "4    [[tensor(1.), tensor(0.), tensor(0.), tensor(0...\n",
       "Name: ohe, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'] = df['rare_words_removed'].apply(one_hot_encoding)\n",
    "df['ohe'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 0., 0., 0., 0.]),\n",
       " tensor([0., 1., 0., 0., 0.]),\n",
       " tensor([0., 0., 1., 0., 0.]),\n",
       " tensor([0., 0., 0., 1., 0.]),\n",
       " tensor([0., 0., 0., 0., 1.])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ohe'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Bag of words**\n",
    "- Example: \"The cat sat on the mat\"\n",
    "- Bag-of-words: {'the': 2, 'cat': 1, 'sat': 1, 'on': 1, 'mat': 1}\n",
    "\n",
    "- Treating each document as an unordered collection of words\n",
    "-  Focuses on frequency, not order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?']\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. TF-IDF**\n",
    "Term Frequency-Inverse Document Frequency\n",
    "- Scores the importance of words in a document\n",
    "- Rare words have a higher score\n",
    "- Common ones have a lower score\n",
    "- Emphasizes informative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This document is the second document.', 'And this is the third one.','Is this the first document?']\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodong Techniques REcap\n",
    "- One hot encoding\n",
    "- Words of bags\n",
    "- TF-IDF encoding\n",
    "- More techniques exist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
